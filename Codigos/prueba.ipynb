{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The file exists\n",
      " The file exists\n",
      " The file exists\n",
      " The file exists\n",
      " The file exists\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "\n",
    "from models import *\n",
    "from read_cdf import *\n",
    "from variables import *\n",
    "from performance import *\n",
    "from save_models_info import *\n",
    "\n",
    "from plots import time_plot, corr_plot \n",
    "from plots import plot_loss, plot_acc, plot_r2_score, density_plot\n",
    "\n",
    "\n",
    "###### [ Checks Folder ] ######\n",
    "check_folder(save_raw_file)\n",
    "check_folder(processing_file)\n",
    "check_folder(model_file)\n",
    "check_folder(result_file)\n",
    "check_folder(plot_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The file already exists in /home/yerko/Desktop/raw_data/omni_data_1995_to_2018.feather\n"
     ]
    }
   ],
   "source": [
    "###### [ Dataset Building ] ######\n",
    "dataset(in_year, out_year, omni_file, save_feather, processing_file, processOMNI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### [ Read CDF ] ######\n",
    "if os.path.exists(save_feather):\n",
    "    df = pd.read_feather(save_feather)\n",
    "else:\n",
    "    print('The file does not exists')\n",
    "\n",
    "\n",
    "###### [ Plot Option ] ######\n",
    "if processPLOT:\n",
    "    print('---- [ Statistical graphs are being created ] ----')\n",
    "    time_plot(df, in_year, out_year, auroral_param, plot_file)\n",
    "    print('[ Time Serie Plot has been created ]')\n",
    "    corr_plot(df, correlation, plot_file) \n",
    "    print('[ Correlation Plot has been created ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------- [ Porcentage Set ] ----------\n",
      "\n",
      "Porcentage Train Set: 60.0% \n",
      "Porcentage Valid Set: 20.0% \n",
      "Porcentage Test Set: 20.0% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### [ Scaler ] ######\n",
    "df = scaler_df(df, scaler, omni_param, auroral_param)\n",
    "\n",
    "\n",
    "###### [ Porcentage Set ] ######\n",
    "if storm_list:\n",
    "    df_train = create_group_prediction(df, omni_param, auroral_param, 'train', storm_list, n_split_train_val_test, n_split_train_val, processing_file)\n",
    "    df_val = create_group_prediction(df, omni_param, auroral_param, 'val', storm_list, n_split_train_val_test, n_split_train_val, processing_file)\n",
    "    df_test = create_group_prediction(df, omni_param, auroral_param, 'test', storm_list, n_split_train_val_test, n_split_train_val, processing_file)\n",
    "\n",
    "else:\n",
    "    df_train, df_val, df_test = create_group_prediction(df, omni_param, auroral_param, 'non', storm_list, n_split_train_val_test, n_split_train_val, processing_file)\n",
    "\n",
    "train_len = round(len(df_train)/len(df),2) * 100\n",
    "val_len = round(len(df_val)/len(df),2) * 100\n",
    "test_len = round(len(df_test)/len(df),2) * 100\n",
    "\n",
    "print()\n",
    "print('---------- [ Porcentage Set ] ----------')\n",
    "print()\n",
    "print(f'Porcentage Train Set: {train_len}% ')\n",
    "print(f'Porcentage Valid Set: {val_len}% ')\n",
    "print(f'Porcentage Test Set: {test_len}% ')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- [ Dimension Set ] ----------\n",
      "\n",
      "Dimension Train Set: OMNI--> (7336221, 44)  |    AE Index--> (7336221,)\n",
      "Dimension Valid Set: OMNI--> (2445405, 44)  |    AE Index--> (2445405,)\n",
      "Dimension Test Set: OMNI--> (2445405, 44)  |    AE Index--> (2445405,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### [ Shift o Delay ] ######\n",
    "if type_model == 'ANN':\n",
    "    omni_train, index_train = shifty_1d(df_train, omni_param, auroral_index, shifty)\n",
    "    omni_val, index_val = shifty_1d(df_val, omni_param, auroral_index, shifty)\n",
    "    omni_test, index_test = shifty_1d(df_test, omni_param, auroral_index, shifty)\n",
    "else:\n",
    "    omni_train, index_train = shifty_3d(df_train, omni_param, auroral_index, shifty)\n",
    "    omni_val, index_val = shifty_3d(df_val, omni_param, auroral_index, shifty)\n",
    "    omni_test, index_test = shifty_3d(df_test, omni_param, auroral_index, shifty)   \n",
    "\n",
    "\n",
    "print('---------- [ Dimension Set ] ----------')\n",
    "print()\n",
    "print(f'Dimension Train Set: OMNI--> {omni_train.shape}  |    {auroral_index.replace(\"_INDEX\", \" Index\")}--> {index_train.shape}')\n",
    "print(f'Dimension Valid Set: OMNI--> {omni_val.shape}  |    {auroral_index.replace(\"_INDEX\", \" Index\")}--> {index_val.shape}')\n",
    "print(f'Dimension Test Set: OMNI--> {omni_test.shape}  |    {auroral_index.replace(\"_INDEX\", \" Index\")}--> {index_test.shape}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### [ DataTorch and DataLoad ] ######\n",
    "train = CustomDataset(omni_train, index_train, device)\n",
    "val = CustomDataset(omni_val, index_val, device)\n",
    "test = CustomDataset(omni_test, index_test, device)\n",
    "\n",
    "train_loader = DataLoader(train, shuffle=True, batch_size=batch_train_val)\n",
    "val_loader = DataLoader(val, shuffle=False, batch_size=batch_train_val)\n",
    "test_loader = DataLoader(test, shuffle=False, batch_size=barch_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- [ ANN ] -----\n",
      "----- [ Neural Network Model ] -----\n",
      "<bound method Module.parameters of ANN(\n",
      "  (drop1): Dropout(p=0.2, inplace=False)\n",
      "  (drop2): Dropout(p=0.2, inplace=False)\n",
      "  (drop3): Dropout(p=0.2, inplace=False)\n",
      "  (relu1): ReLU()\n",
      "  (relu2): ReLU()\n",
      "  (relu3): ReLU()\n",
      "  (relu4): ReLU()\n",
      "  (fc1): Linear(in_features=44, out_features=320, bias=True)\n",
      "  (fc2): Linear(in_features=320, out_features=160, bias=True)\n",
      "  (fc3): Linear(in_features=160, out_features=160, bias=True)\n",
      "  (fc4): Linear(in_features=160, out_features=80, bias=True)\n",
      "  (fc5): Linear(in_features=80, out_features=20, bias=True)\n",
      "  (fc6): Linear(in_features=20, out_features=5, bias=True)\n",
      "  (fc7): Linear(in_features=5, out_features=1, bias=True)\n",
      ")>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### [ Neural Network Model ] ######\n",
    "if type_model == 'ANN':\n",
    "    print('----- [ ANN ] -----')\n",
    "    input_size = omni_train.shape[1]\n",
    "    model = ANN(input_size=input_size, drop=drop).to(device)\n",
    "if type_model == 'LSTM':\n",
    "    print('----- [ LSTM ] -----')\n",
    "    input_size = omni_train.shape[2]\n",
    "    model = LSTM(input_size=input_size, drop=drop, num_layer=num_layer, device=device).to(device)\n",
    "\n",
    "if type_model == 'CNN':\n",
    "    print('----- [ CNN ] -----')\n",
    "    #model = CNN(input_size=input_size, drop=drop).to(device)\n",
    "\n",
    "print('----- [ Neural Network Model ] -----')\n",
    "print(model.parameters)\n",
    "print()\n",
    "\n",
    "\n",
    "###### [ HyperParameters ] ######\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ [ Start to Training Model ] ------\n",
      "\n",
      "Epoch 10/300: Train loss (RMSE): 227.601, Train acc: 0.003, Train R2 Score: -0.076 | Valid loss (RMSE): 136.055, Valid acc: 0.009, Valid R2 Score: 0.395\n",
      "Epoch 20/300: Train loss (RMSE): 158.310, Train acc: 0.004, Train R2 Score: 0.480 | Valid loss (RMSE): 119.964, Valid acc: 0.006, Valid R2 Score: 0.530\n",
      "Epoch 30/300: Train loss (RMSE): 156.371, Train acc: 0.004, Train R2 Score: 0.492 | Valid loss (RMSE): 118.270, Valid acc: 0.006, Valid R2 Score: 0.543\n",
      "Epoch 40/300: Train loss (RMSE): 155.819, Train acc: 0.004, Train R2 Score: 0.496 | Valid loss (RMSE): 117.883, Valid acc: 0.006, Valid R2 Score: 0.546\n",
      "Epoch 50/300: Train loss (RMSE): 155.403, Train acc: 0.004, Train R2 Score: 0.498 | Valid loss (RMSE): 117.607, Valid acc: 0.006, Valid R2 Score: 0.548\n",
      "Epoch 60/300: Train loss (RMSE): 155.123, Train acc: 0.004, Train R2 Score: 0.500 | Valid loss (RMSE): 117.614, Valid acc: 0.006, Valid R2 Score: 0.548\n",
      "Epoch 70/300: Train loss (RMSE): 154.903, Train acc: 0.005, Train R2 Score: 0.502 | Valid loss (RMSE): 117.294, Valid acc: 0.006, Valid R2 Score: 0.550\n",
      "Epoch 80/300: Train loss (RMSE): 154.745, Train acc: 0.004, Train R2 Score: 0.503 | Valid loss (RMSE): 117.216, Valid acc: 0.006, Valid R2 Score: 0.551\n",
      "Epoch 90/300: Train loss (RMSE): 154.599, Train acc: 0.004, Train R2 Score: 0.504 | Valid loss (RMSE): 117.227, Valid acc: 0.006, Valid R2 Score: 0.551\n",
      "Epoch 100/300: Train loss (RMSE): 154.476, Train acc: 0.004, Train R2 Score: 0.504 | Valid loss (RMSE): 117.159, Valid acc: 0.006, Valid R2 Score: 0.551\n",
      "Epoch 110/300: Train loss (RMSE): 154.417, Train acc: 0.004, Train R2 Score: 0.505 | Valid loss (RMSE): 117.106, Valid acc: 0.006, Valid R2 Score: 0.552\n",
      "Epoch 120/300: Train loss (RMSE): 154.154, Train acc: 0.004, Train R2 Score: 0.506 | Valid loss (RMSE): 117.125, Valid acc: 0.006, Valid R2 Score: 0.552\n",
      "Epoch 130/300: Train loss (RMSE): 154.052, Train acc: 0.004, Train R2 Score: 0.507 | Valid loss (RMSE): 117.037, Valid acc: 0.006, Valid R2 Score: 0.552\n",
      "Epoch 140/300: Train loss (RMSE): 153.935, Train acc: 0.004, Train R2 Score: 0.508 | Valid loss (RMSE): 116.819, Valid acc: 0.006, Valid R2 Score: 0.554\n",
      "Epoch 150/300: Train loss (RMSE): 153.903, Train acc: 0.004, Train R2 Score: 0.508 | Valid loss (RMSE): 117.054, Valid acc: 0.006, Valid R2 Score: 0.552\n",
      "Epoch 160/300: Train loss (RMSE): 153.715, Train acc: 0.004, Train R2 Score: 0.509 | Valid loss (RMSE): 116.816, Valid acc: 0.006, Valid R2 Score: 0.554\n",
      "Epoch 170/300: Train loss (RMSE): 153.550, Train acc: 0.004, Train R2 Score: 0.510 | Valid loss (RMSE): 116.911, Valid acc: 0.006, Valid R2 Score: 0.553\n",
      "Epoch 180/300: Train loss (RMSE): 153.354, Train acc: 0.005, Train R2 Score: 0.512 | Valid loss (RMSE): 116.948, Valid acc: 0.006, Valid R2 Score: 0.553\n",
      "Epoch 190/300: Train loss (RMSE): 153.249, Train acc: 0.004, Train R2 Score: 0.512 | Valid loss (RMSE): 116.894, Valid acc: 0.006, Valid R2 Score: 0.553\n",
      "Epoch 200/300: Train loss (RMSE): 153.142, Train acc: 0.004, Train R2 Score: 0.513 | Valid loss (RMSE): 117.034, Valid acc: 0.006, Valid R2 Score: 0.552\n",
      "Epoch 210/300: Train loss (RMSE): 152.987, Train acc: 0.004, Train R2 Score: 0.514 | Valid loss (RMSE): 117.026, Valid acc: 0.006, Valid R2 Score: 0.552\n",
      "Epoch 220/300: Train loss (RMSE): 152.936, Train acc: 0.004, Train R2 Score: 0.514 | Valid loss (RMSE): 116.992, Valid acc: 0.006, Valid R2 Score: 0.553\n",
      "Epoch 230/300: Train loss (RMSE): 152.830, Train acc: 0.004, Train R2 Score: 0.515 | Valid loss (RMSE): 116.822, Valid acc: 0.006, Valid R2 Score: 0.554\n",
      "Epoch 240/300: Train loss (RMSE): 152.799, Train acc: 0.004, Train R2 Score: 0.515 | Valid loss (RMSE): 116.990, Valid acc: 0.006, Valid R2 Score: 0.553\n",
      "Epoch 250/300: Train loss (RMSE): 152.688, Train acc: 0.004, Train R2 Score: 0.516 | Valid loss (RMSE): 117.117, Valid acc: 0.006, Valid R2 Score: 0.552\n",
      "Epoch 260/300: Train loss (RMSE): 152.646, Train acc: 0.004, Train R2 Score: 0.516 | Valid loss (RMSE): 116.941, Valid acc: 0.006, Valid R2 Score: 0.553\n",
      "Epoch 270/300: Train loss (RMSE): 152.566, Train acc: 0.004, Train R2 Score: 0.517 | Valid loss (RMSE): 117.160, Valid acc: 0.006, Valid R2 Score: 0.551\n",
      "Epoch 280/300: Train loss (RMSE): 152.453, Train acc: 0.004, Train R2 Score: 0.517 | Valid loss (RMSE): 117.221, Valid acc: 0.006, Valid R2 Score: 0.551\n",
      "Epoch 290/300: Train loss (RMSE): 152.391, Train acc: 0.004, Train R2 Score: 0.518 | Valid loss (RMSE): 116.996, Valid acc: 0.006, Valid R2 Score: 0.553\n",
      "Epoch 300/300: Train loss (RMSE): 152.287, Train acc: 0.004, Train R2 Score: 0.518 | Valid loss (RMSE): 117.134, Valid acc: 0.006, Valid R2 Score: 0.552\n",
      "Test Loss 146.3542 | Test Accuracy: 0.0050 | Test R2 Score: 0.4676\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Lets Go! You win, the model has been trained, validated and tested\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### [ Training/Validation/Test Process ] ######\n",
    "start_time = time.time()\n",
    "print('------ [ Start to Training Model ] ------')\n",
    "print()\n",
    "\n",
    "train_loss, val_loss, test_loss = [], [], []\n",
    "train_acc, val_acc, test_acc = [], [], []\n",
    "train_r2, val_r2, test_r2 = [], [], []\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    ### [ Training ] ###\n",
    "    model.train()\n",
    "    count_train_loss = 0\n",
    "    label_train_real, label_train_pred = [], []\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(x)\n",
    "        loss = criterion(yhat, y)  # Usar MSELoss directamente\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        count_train_loss += loss.item()\n",
    "        label_train_real.append(y.detach().cpu().numpy())\n",
    "        label_train_pred.append((yhat.detach().cpu().numpy()))\n",
    "\n",
    "    train_real = pd.Series(np.concatenate(label_train_real).astype(\"float\").flatten().reshape(-1))\n",
    "    train_pred = pd.Series(np.round(np.concatenate(label_train_pred).astype(\"float\").flatten()))\n",
    "\n",
    "    train_accuracy = accuracy_score(train_real, train_pred)\n",
    "    train_r2_score = r2_score(train_real, train_pred)\n",
    "    \n",
    "    # Calcular RMSE\n",
    "    train_rmse = np.sqrt(count_train_loss / len(train_loader))\n",
    "    train_loss.append(train_rmse)\n",
    "    train_acc.append(train_accuracy)\n",
    "    train_r2.append(train_r2_score)\n",
    "\n",
    "    ### [ Validation ] ###\n",
    "    model.eval()\n",
    "    count_val_loss = 0\n",
    "    label_val_real, label_val_pred = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            yhat = model(x)\n",
    "            loss = criterion(yhat, y)\n",
    "\n",
    "            count_val_loss += loss.item()\n",
    "            label_val_real.append(y.detach().cpu().numpy())\n",
    "            label_val_pred.append(yhat.detach().cpu().numpy())\n",
    "\n",
    "    val_real = pd.Series(np.concatenate(label_val_real).astype(\"float\").flatten().reshape(-1))\n",
    "    val_pred = pd.Series(np.round(np.concatenate(label_val_pred).astype(\"float\").flatten()))\n",
    "    val_accuracy = accuracy_score(val_real, val_pred)\n",
    "    val_r2_score = r2_score(val_real, val_pred)\n",
    "    \n",
    "    # Calcular RMSE\n",
    "    val_rmse = np.sqrt(count_val_loss / len(val_loader))\n",
    "    val_loss.append(val_rmse)\n",
    "    val_acc.append(val_accuracy)\n",
    "    val_r2.append(val_r2_score)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch+10}/{num_epoch}: '\n",
    "              f'Train loss (RMSE): {train_loss[-1]:.3f}, Train acc: {train_accuracy:.3f}, Train R2 Score: {train_r2_score:.3f} | '\n",
    "              f'Valid loss (RMSE): {val_loss[-1]:.3f}, Valid acc: {val_accuracy:.3f}, Valid R2 Score: {val_r2_score:.3f}')\n",
    "\n",
    "    today = datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "    if count_val_loss / len(val_loader) >= early_stop:\n",
    "        print(f'Validation Loss value {val_loss[-1]:.4f} reached the limit in epoch {epoch}')\n",
    "        num_epoch = epoch + 1\n",
    "        break\n",
    "\n",
    "### [ Save Model ] ###\n",
    "torch.save(model.state_dict(), model_file + f'Model_{type_model}_{auroral_index}_{today}_Epoch_{num_epoch}.pt')\n",
    "\n",
    "### [ Test ] ###\n",
    "model.eval()\n",
    "count_test_loss = 0\n",
    "label_test_real, label_test_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        yhat = model(x)\n",
    "        loss = criterion(yhat, y)\n",
    "\n",
    "        count_test_loss += loss.item()\n",
    "        label_test_real.append(y.detach().cpu().numpy())\n",
    "        label_test_pred.append(yhat.detach().cpu().numpy())\n",
    "\n",
    "test_real = pd.Series(np.concatenate(label_test_real).astype(\"float\").flatten().reshape(-1))\n",
    "test_pred = pd.Series(np.round(np.concatenate(label_test_pred).astype(\"float\").flatten()))\n",
    "test_accuracy = accuracy_score(test_real, test_pred)\n",
    "test_r2_score = r2_score(test_real, test_pred)\n",
    "\n",
    "# Calcular RMSE\n",
    "test_rmse = np.sqrt(count_test_loss / len(test_loader))\n",
    "test_loss.append(test_rmse)\n",
    "test_acc.append(test_accuracy)\n",
    "test_r2.append(test_r2_score)\n",
    "\n",
    "print(f'Test Loss {test_loss[-1]:.4f} | Test Accuracy: {test_accuracy:.4f} | Test R2 Score: {test_r2_score:.4f}')  \n",
    "print()\n",
    "print('---------------------------------------------------------------------')\n",
    "print('Lets Go! You win, the model has been trained, validated and tested')\n",
    "print()\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "diff_timedelta =timedelta(seconds=total_time)\n",
    "format_diff = str(diff_timedelta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [ Loss Plot ] ---\n",
      "--- [ Accuracy Plot ] ---\n",
      "--- [ Accuracy Plot ] ---\n",
      "--- [ Density Plot ] ---\n"
     ]
    }
   ],
   "source": [
    "#### [ Plot Model ] ####\n",
    "plot_loss(num_epoch, train_loss, val_loss, plot_file, type_model, auroral_index)\n",
    "plot_acc(num_epoch, train_acc, val_acc, plot_file, type_model, auroral_index)\n",
    "plot_r2_score(num_epoch, train_r2, val_r2, plot_file, type_model, auroral_index)\n",
    "density_plot(test_real, test_pred, plot_file, type_model, auroral_index, 'test') ### A mejorar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### [ Save Info Model ] ####\n",
    "save_model_txt(model, num_epoch, criterion, optimizer, train_loss, val_loss, test_loss, train_acc, val_acc, test_acc, train_r2, val_r2, test_r2, \n",
    "                df_train, df_val, df_test, format_diff, auroral_index, omni_param, type_model, learning_rate, scaler, in_year, out_year, model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
